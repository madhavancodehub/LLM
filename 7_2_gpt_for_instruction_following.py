# -*- coding: utf-8 -*-
"""7.2 GPT for Instruction Following.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OUHnyQevDJA1p_tDDUqfWdxKfwRCz1Xt

# ğŸ¤– **Training GPT for Instruction Following**

In this notebook, we demonstrate the process of training a GPT model to follow instructions effectively, using a custom dataset for varied tasks. Let's embark on this journey from data preprocessing to model training and text generation.

## Setup and Dependencies

First, we import necessary libraries and set up our environment to handle the tasks.
"""

!pip install transformers[torch]==4.38.2
!pip install datasets===2.13.1

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)
from datasets import load_dataset

"""## Data Exploration and Preparation ğŸ“Š

Let's load and preview our dataset, ensuring we understand the kind of data we're working with.

## Data Exploration and Preparation ğŸ“Š

Let's load and preview our dataset, ensuring we understand the kind of data we're working with.
"""

dataset = load_dataset("hakurei/open-instruct-v1", split='train')
dataset.to_pandas().sample(20)

def preprocess(example):
    example['prompt'] = f"{example['instruction']} {example['input']} {example['output']}"

    return example

def tokenize_datasets(dataset):
    tokenized_dataset = dataset.map(lambda example: tokenizer(example['prompt'], truncation=True, max_length=128), batched=True, remove_columns=['prompt'])

    return tokenized_dataset

"""
### Shuffling and Splitting the Dataset

Next, we shuffle the dataset and split it into training and test sets to ensure robust model training and evaluation."""

dataset = dataset.map(preprocess, remove_columns=['instruction', 'input', 'output'])
dataset =  dataset.shuffle(42).select(range(100000)).train_test_split(test_size=0.1, seed=42)

train_dataset = dataset['train']
test_dataset = dataset['test']

"""## Model Initialization and Tokenization ğŸš€

We set up the tokenizer and the model, ensuring that our tokens align with the model's expected format.
"""

MODEL_NAME = "microsoft/DialoGPT-medium"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

train_dataset = tokenize_datasets(train_dataset)
test_dataset = tokenize_datasets(test_dataset)

model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

"""## Training the GPT Model ğŸ¯
Now, we configure the training parameters and initiate the training process using our prepared datasets.
"""

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

traing_args = TrainingArguments(output_dir="models/diablo_gpt",
                                num_train_epochs=1,
                                per_device_train_batch_size=32,
                                per_device_eval_batch_size=32)\

trainer = Trainer(model=model,
                    args=traing_args,
                    train_dataset=train_dataset,
                    eval_dataset=test_dataset,
                    data_collator=data_collator)

trainer.train() # this will take a long time

# Get the trained checkpoint directly
model = AutoModelForCausalLM.from_pretrained("TheFuzzyScientist/diabloGPT_open-instruct")

"""## Text Generation and Application ğŸ“
Finally, let's demonstrate the capability of our trained model by generating responses to various instructions.
"""

def generate_text(prompt):
    inputs = tokenizer.encode(prompt, return_tensors='pt') #.to("cuda") # <-- if running on GPU, uncomment this
    outputs = model.generate(inputs, max_length=64, pad_token_id=tokenizer.eos_token_id)
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return generated[:generated.rfind('.')+1]

generate_text("What's the best way to cook chiken breast?")

generate_text("Should I invest stocks?")

generate_text("I need a place to go for this summer vacation, what locations would you recommend")

generate_text("What's the fastest route from NY City to Boston?")

"""
## Conclusion and Further Exploration ğŸŒŸ

This notebook guides you through training a GPT model to follow complex instructions. Experiment further by altering the dataset or tweaking the model and training configurations to explore new possibilities!"""